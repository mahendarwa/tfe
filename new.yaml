# This policy uses the tfplan/v2 import to get the Terraform plan data and checks if
# workload_metadata_config is set to GKE_METADATA for all node pools.
# It will also validate that workload_identity_config has a project name that follows allowed naming convention
# (old: contains "prj", or new: matches ge[0-9]+-gke-[a-z0-9]+)

# Import common-functions/tfplan-functions.sentinel
# with alias "plan"
import "tfplan-functions" as plan

metadata = {
    "version": "1.0.0",
    "category": "GKE",
    "priority": "High",
    "customComplianceCacRef": "GCPPLAT-GKE-08",
    "createdBy": "td-cac-team",
    "policyDescription": "Ensure using dedicated GCP Service Accounts and Workload Identity",
    "policyName": "GCP-TFSENTINEL-GKE-120-002.sentinel",
}

# Get all GKE clusters
allGKENodePools = plan.find_resources("google_container_node_pool")
allGKEClusters = plan.find_resources("google_container_cluster")

violating_nodepools = 0
violating_clusters = 0
nondefault_nodepools = 0

# Start cluster checks
for allGKEClusters as address, r {

    workload_identity_config = r.change.after.workload_identity_config[0] else null

    if workload_identity_config is null or workload_identity_config == [] {
        print("invalid", r.address, "workload_identity_config{} is not set.")
        violating_clusters += 1
    } else {
        workload_pool = workload_identity_config.workload_pool else null

        ### ADDED LINE ###
        valid_new_pattern = "ge[0-9]+-gke-[a-z0-9]+"
        ### ADDED LINE ###

        ### REPLACED OLD BLOCK ###
        if (workload_pool == null) or 
           (not workload_pool contains "prj" and not regex.match(valid_new_pattern, workload_pool)) {
            print("invalid", r.address, "workload_pool does not match allowed naming convention, received:", workload_pool)
            violating_clusters += 1
        }
        ### REPLACED OLD BLOCK ###
    }

    # Check for remove_default_node_pool
    if r.change.after.remove_default_node_pool != true {
        print("invalid", r.address, "remove_default_node_pool expected true, but received:", r.change.after.remove_default_node_pool)
        violating_clusters += 1
    }
}

# Start node pool checks
for allGKENodePools as address, r {

    if r.index == "default-node-pool" {
        # Skip default node pool
        continue
    }

    nondefault_nodepools += 1

    workload_metadata_config = r.change.after.node_config[0].workload_metadata_config else null
    metadata_mode = workload_metadata_config[0].mode else null

    if workload_metadata_config is null {
        print("invalid", r.address, "node_config.0.workload_metadata_config{} is not set.")
        violating_nodepools += 1
    } else if metadata_mode != "GKE_METADATA" {
        print("invalid", r.address, "resource, mode expected GKE_METADATA, but received:", metadata_mode)
        violating_nodepools += 1
    }
}

# Final check: must have at least one non-default node pool
if nondefault_nodepools < length(allGKEClusters) {
    violating_nodepools += 1
    print("Expected at least one non-default node pool to be created")
}

# Main rule result
main = rule {
    violating_clusters is 0 and violating_nodepools is 0
}
